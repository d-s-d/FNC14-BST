\section{Optimization}
\mypar{Turn Columns into Rows} Examining the innermost loop, we see that the
the table is accessed row- and column-wise. Thus, the memory is accessed in
strides of $1$ and $n+1$, respectively. However, with two simple changes to the
algorithm, we can avoid strides of $n+1$ and replace them with accesses of
stride $1$: We make use of the lower half of the square table in that we store
newly calculated values not only at $(i,j)$ but also at $(j,i)$. That is,
instead of accessing a column in the upper half, we access a row in the
lower-half. This needs only two minor changes to the algorithm as described in
listing \ref{lst:baseline}: First, line $7$ turns into
\begin{center}
\verb:t = e[IDX(i,r)] + e[IDX(j,r+1)]:, 
\end{center}
and after line $10$, we would insert 
\begin{center}
	\verb:e[IDX(j,i)] = t;:.
\end{center}
\mypar{Bottom-Up} The order in which individual values are calculated is along
the diagonals, as visualized in Figure TODO. We notice that, for two distinct
cells on a diagonal, their corresponding rows and colums intersect exactly in
one cell. By changing the outermost two loops such that the table is being built
up row-wise and bottom-up (as visualized in Figure TODO) we can thus improve the
tomporal locality with respect to the accesses to the row that is currently
being built-up.

\mypar{Partial Results} We can further increase the temporal locality by
swapping the innermost two loops. Conceptually speaking, instead of calculating
the minimum value over a full row and column, we store intermediate values for
an entire row. That is, we take the first value, let it be $q_i$, add it to
all the values stored in the row $i+1$, compare it to the .

\mypar{Compressed memory layout}
\mypar{Blocking}
\mypar{Vectorization}
\mypar{Scheduling}
\mypar{Alignment}


