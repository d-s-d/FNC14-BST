\section{Background}

As a group we worked on the static optimal binary search tree problem.
In this section we give a mathematical description of the optimal binary
search tree problem. The algorithm implemented and optimized to solve the
problem is presented and a cost measure introduced. The presentation is
based on the algorithms book by Cormen et al.~\cite{MITBook}. We conclude
by giving a short overview of two asymptotically faster algorithms.

\mypar{Problem Statement} Let $K = \{k_1, k_2, \dots, k_n\}$ be a sequence
of distinct ordered keys. For each key $k_i$, let $p_i$ be the probability
that a given search is for key $k_i$. Such a search is called successfull.
Furhter, let $D = \{d_0, d_1, \dots, d_n\}$ be the set of dummy keys
returned for unsuccessfull searches as follows: $d_i$ represents searches
for values between $k_i$ and $k_{i+1}$, $d_0$ represent the values smaller
$k_0$ and $d_n$ the values larger $k_n$. For each dummy key $d_i$, let
$q_i$ be the probability that a given search returns $d_i$.
A valid solution to the static binary search tree problem is any binary
search tree $T$ that has $K$ as nodes and $D$ as leaves.
The cost of a search in a such a tree $T$ is defined as the depth of the key
found plus one. Since $K$ and $D$ cover all possible searches,
$\sum_{i=1}^n p_i + \sum_{j=0}^n q_j = 1$ and we can compute the expected
cost of a search in $T$ as:
\begin{align}
  %\mathbb{E}[\text{search cost in } T] =
  %\nonumber\\
  %\mathcal{E}_T &=
  \sum_{i=1}^n (\depth_T(k_i) + 1) \cdot p_i
   + \sum_{i=0}^n (\depth_T(d_i) + 1) \cdot q_i
  \nonumber\\
  % I just can't fit the formula to width :-(
  = 1 + \sum_{i=1}^n \depth_T(k_i) \cdot p_i
      + \sum_{i=0}^n \depth_T(d_i) \cdot q_i
  \label{eqn:cost}
\end{align}
A static binary search tree is called optimal if its expected search cost
is minimal amongst all valid solution trees.
We can now formulate the static optimal binary search tree problem as
follows: Given $K$, $P$ and $Q$, find the optimal binary search tree.

\mypar{Algorithm} Devising an algorithm to solve the problem requires a
crucial insight on the problem structure: Given an optimal tree $T$ for
keys $k_1$ to $k_n$ with root $k_r$, its left subtree $T_L$ is an optimal
solution for the keys $k_1$ to $k_{r-1}$. Clearly, it has to be a binary
search tree for these keys. Then, if it were not optimal, one could replace
$T_L$ by an optimal binary search tree for the keys $k_1,\dots,k_{r-1}$
obtaining a better solution $T'$ for the original problem. However, this
contradicts the optimality of $T$ and hence $T_L$ is optimal as well. This
argument holds anologous for all subtrees in $T$.

From this insight we can construct an algorithm. Let $e[i,j]$ denote the
expected search cost in the optimal binary search tree continaing keys
$k_i$ through $k_j$. If such a tree is used as a left or right subtree to
construct a tree containing more keys, the depth of its nodes increases by
one. Following \autoref{eqn:cost}, the subtree's expected search cost
increases by
\begin{align}
  w(i,j) &= \sum_{l=1}^j p_l + \sum_{l=i-1}^j q_l \nonumber\\
         &= w(i,r-1) + p_r + w(r+1,j)
  \label{eqn:w}
\end{align}
where the second expression reflects the recursive structure of the problem
again. This allows to express the search cost of a binary search tree over
$k_i$ to $k_j$ with root $k_r$ constructed from its subtrees as
\begin{align}
  e[i,j] = e[i,r-1] + e[r+1,j] + w(i,j)
  \label{eqn:e-intermediate}
\end{align}
Knowing the optimal expected search costs for all possible subtrees, we can
construct the optimal binary search tree by choosing the key as root that
minimizes \autoref{eqn:e-intermediate}, i.e
\footnote{Note that for simplicity, we do not include border cases here.
For a complete discussion of the algorithm and its mathematics refer to
\cite{MITBook}.}
\begin{align}
  e[i,j] = \min_{i\leq r\leq j} \{e[i,r-1] + e[r+1,j] + w(i,j)\}
  \label{eqn:e}
\end{align}
This expression can now direclty be translated into code, as is shown in
\autoref{lst:baseline}. The cost of the subtrees is computed using dynamic
programming. The table \texttt{e[IDX(i,j)]} is used to store the
expected search cost for the optimal search tree covering keys in $k_i$
through $k_j$\footnote{Note that the code actually uses zero-indexing.}. We
fill the table \texttt{e} diagonal by diagonal. This corresponds to
computing first all the subrees continaing one key, then containing two
keys and so forth, as indicated by the length variable \texttt{l}. The
innermost \texttt{r}-loop iterates over all valid roots for the subtree to
find the optimal binary search tree for the current keys. We refer to the
expected cost \texttt{e[IDX(i,j)]} to be computed as the \emph{target
cell}. A second table called \texttt{root} is used to keep track which root
was chosen for the current subtree to be able to reconstruct the overall
optimal tree in the end. In the code section, we have omitted the
initialization code. The computation is dominated by the triple loop shown.
\begin{lstlisting}[
  caption={Basline Implementation},
  label=lst:baseline,
  float
]
for (l = 1; l < n+1; l++)
  for (i = 0; i < n-l+1; i++)
    j = i+l;
    e[IDX(i,j)] = INFINITY;
    w[IDX(i,j)] = w[IDX(i,j-1)] + p[j-1] + q[j];
    for (r = i; r < j; r++ )
      t = e[IDX(i,r)] + e[IDX(r+1,j)]
          + w[IDX(i,j)];
      if (t < e[IDX(i,j)])
        e[IDX(i,j)]    = t;
        root[IDX(i,j)] = r;
\end{lstlisting}

\mypar{Cost Analysis} The presented algorithm has runtime $O(n^3)$. As can
be seen in \autoref{lst:baseline}, the computation involves additions and
comparisions only. We define the cost function of the algorithm as the
number of floating point additions and floating point comparisons:
\begin{align*}
C(n) = (\#\text{adds}(n), \#\text{comps}(n))
\end{align*}
The body of the second loop is executed a total of
\begin{align*}
\sum_{i=1}^{n} i = \frac{n(n+1)}{2}
\end{align*}
times. The body of the innermost loop is executed a total of
\begin{align*}
\sum_{i=1}^{n} l\cdot(n-l) = \frac{1}{6}(n^3-n)
\end{align*}
times.  Hence, the cost function is defined as:
\begin{align*}
C(n) = \left(\frac{1}{3}(n^3+3n^2+2n), \frac{1}{6}(n^3 - n)\right)
\end{align*}

\mypar{Alternative Algorithms} The presented algorithm was originally
published by Knuth. He provides further analysis yielding that under
certain conditions the root will not change, allowing to neglect some of
the \texttt{r}-iterations of the algorithm. The final algorithm has runtime
$O(n^2)$. Details are found in \cite{Knuth70}. If an approximation of the
result is sufficient, Mehlhorn \cite{Mehlhorn75} showed that balancing
probabilities in the left and right subtrees already yields results close
to optimal.
